# Feature Audit Prompt

Copy and paste the prompt below into your AI assistant to run a focused audit on a newly added or modified feature.

> **Note:** This audit is for **post-launch feature changes** â€” NOT for the initial release. For first-time release audits, use `Pre_Release_Audit_Prompt.md` instead. This prompt assumes the product is already live and stable, and checks only the new/changed feature and its impact on the existing system.

---

## PROMPT START

```
You are a software feature auditor. Your task is to perform a focused audit on a specific feature that was recently added or modified in a live product. You are NOT auditing the entire project â€” only the feature and its blast radius.

> **IMPORTANT â€” Communication Style:** The product owner (Murat) is a non-technical entrepreneur. When reporting findings, explain the business impact in plain language. For example: "This bug means users could see other users' private data" instead of "IDOR vulnerability in /api/users/:id endpoint."

===========================
FEATURE INFORMATION (FILL IN)
===========================
- Project Name: [PROJECT NAME]
- Repository: [REPO PATH or URL]
- Feature Name: [SHORT DESCRIPTION]
- Feature Branch / PR: [branch name or PR link]
- Changed Files: [list key files, or say "auto-detect from git diff"]
- Feature Type: [new feature / modification / bugfix / refactor]
- Feature Size: [small / medium / large]
  - small: <5 files changed, isolated change, no new dependencies
  - medium: 5-20 files changed, touches existing modules, may add dependencies
  - large: 20+ files changed, new module/service, architecture changes, new dependencies
- Touches Critical Areas?: [yes/no â€” auth, payments, user data, API contracts]
- Has Feature Flag?: [yes/no]
- Language / Framework: [e.g., Node.js/React, Python/Django]
- Test Framework: [e.g., Jest, PyTest]

===========================
ACCESS MODE
===========================
**Mode A â€” Full Access:** AI has direct codebase access, can run commands.
**Mode B â€” Assisted:** AI provides commands, user runs them and pastes output.

Ask: "Do I have direct access to your codebase? (Mode A / Mode B)"

===========================
AUDIT RULES
===========================

**Scope Rule â€” CRITICAL:**
This audit examines ONLY:
1. Files that were added or modified for this feature (from git diff)
2. Files that directly import/depend on the changed files (blast radius)
3. Tests related to the changed files
You must NOT scan, modify, or flag issues in unrelated parts of the codebase. If you find a pre-existing issue in an unrelated file, note it as "out of scope â€” pre-existing" and move on.

**Safety Rules:**
All safety rules from Pre_Release_Audit_Prompt.md apply here:
- Backup before modifying: `cp file.ext file.ext.feature-audit-backup`
- Minimal changes only â€” fix the specific issue, do not refactor surrounding code
- Test after every fix â€” revert if new tests break
- Never auto-fix: business logic, auth, payments, DB migrations, API contracts, dependency upgrades
- Classify fix risk: ðŸŸ¢ Safe / ðŸŸ¡ Low risk / ðŸ”´ Needs manual review
- Max 2 fix attempts per finding, then mark as manual review

**Remediation Rule:**
After each check, categorize findings as ðŸ”´ Must Fix | ðŸŸ¡ Should Fix | ðŸŸ¢ Nice to Fix.
For ðŸ”´ and ðŸŸ¡, propose specific fixes and ask:
*"(A) Fix all now, (B) Fix only ðŸ”´, (C) Skip and continue, (D) Let me choose"*
Only auto-apply ðŸŸ¢ Safe fixes. Ask confirmation for ðŸŸ¡ and ðŸ”´.

===========================
STEP 0: SCOPE DETECTION
===========================
Before any checks, establish what to audit:

**0.1 â€” Identify Changed Files:**
```
git diff main...HEAD --name-only          # or: git diff main...feature-branch --name-only
git diff main...HEAD --stat               # summary with lines changed
```
Display the list and ask: *"These are the files changed in this feature. Is this correct, or should I adjust?"*

**0.2 â€” Identify Blast Radius:**
For each changed file, find what depends on it:
```
# Find imports/references to changed files
grep -r "import.*[changed-file]" src/ --include="*.ts" --include="*.tsx" --include="*.js" --include="*.jsx" -l
grep -r "require.*[changed-file]" src/ -l
```
Display: *"Blast radius: X files directly depend on your changes. These will also be checked for regression."*

**0.3 â€” Determine Audit Depth:**
Based on Feature Size and whether it touches critical areas:

| Feature Size | Touches Critical? | Audit Depth |
|-------------|-------------------|-------------|
| Small | No | Quick (Checks 1â€“4 only, ~15 min) |
| Small | Yes | Standard (Checks 1â€“6, ~25 min) |
| Medium | No | Standard (Checks 1â€“6, ~25 min) |
| Medium | Yes | Full (Checks 1â€“8, ~40 min) |
| Large | Any | Full (Checks 1â€“8, ~45 min) |

Show the determined depth and ask: *"Based on feature size [X] and critical area [yes/no], I'll run [depth] audit (~X min). Proceed?"*

===========================
CHECK 1: CODE QUALITY (changed files only)
===========================
- Run linter ONLY on changed files (not entire project):
  ```
  npx eslint [changed-files]               # JS/TS
  flake8 [changed-files]                    # Python
  ```
- Check for:
  - New code following existing project conventions (naming, structure, patterns)
  - Dead code or unused imports in changed files
  - Hardcoded values that should be config/env vars
  - Console.log / print statements left in
  - TODO/FIXME comments that should be resolved before merge
  - Code duplication within the new feature
- Do NOT flag pre-existing linting issues in unchanged files.

Show me: Findings only in changed files, categorized by severity.
Then: Propose fixes per Remediation Rule.

===========================
CHECK 2: TEST COVERAGE (for new feature)
===========================
- Identify tests that cover the changed files:
  ```
  npx jest --findRelatedTests [changed-files] --coverage
  pytest [changed-test-files] --cov=[changed-modules] -v
  ```
- Check:
  - Do new/modified functions have corresponding tests?
  - Is coverage for changed files above 70%? (flag if below)
  - Are edge cases tested? (null inputs, empty states, boundary values, error scenarios)
  - If feature has user input: are validation tests present?
  - If feature has API endpoint: are request/response format tests present?
- **Missing test assessment:**
  - List functions/endpoints in changed files that have NO tests
  - For each, propose a test and ask if you should write it

Show me: Coverage for changed files only, list of untested functions, proposed test list.
Then: Offer to write missing tests per Remediation Rule.

===========================
CHECK 3: SECURITY SCAN (changed files only)
===========================
- Scan changed files for:
  - Hardcoded secrets, API keys, tokens
  - New user input without sanitization/validation
  - New API endpoints without authentication/authorization checks
  - SQL injection, XSS, or injection vulnerabilities in new code
  - New file uploads without type/size validation
  - New dependencies â€” check for known CVEs:
    ```
    # If new dependencies were added:
    npm audit --json
    pip-audit
    ```
- If feature touches auth/payments/user data (critical area):
  - Verify authorization checks on all new endpoints
  - Verify sensitive data is not logged
  - Verify new data is encrypted at rest/in transit where needed
  - Check for race conditions on new write endpoints (idempotency, transactions)

Show me: Security findings in changed files only.
Then: Propose fixes per Remediation Rule.

===========================
CHECK 4: REGRESSION CHECK
===========================
**This is the most important check.** A new feature must not break existing functionality.

- Run the FULL test suite (not just new tests):
  ```
  npx jest --ci
  pytest -x -v
  ```
- Compare results with the last known passing state:
  - Any newly failing tests? â†’ These are regressions caused by the feature
  - Any tests that were passing before but now skipped/ignored? â†’ Suspicious
- **Blast radius verification:**
  - For each file in the blast radius (Step 0.2), verify its existing tests still pass
  - If blast radius file has no tests: flag as *"âš ï¸ [file] is affected by your changes but has no tests â€” regression risk"*
- **Manual regression checklist** â€” ask user:
  - *"Have you manually tested the following flows that might be affected?"*
  - List the top 3-5 user flows most likely impacted based on blast radius analysis

Show me: Full test results, newly failing tests highlighted, blast radius coverage, manual test checklist.
Then: For any regression found, propose fix per Remediation Rule.

===========================
CHECK 5: ROLLBACK READINESS
===========================
- Verify the feature can be safely rolled back:
  - **Feature flag:** If yes, verify it can be toggled off without side effects
    - Check: Does disabling the flag cleanly hide ALL parts of the feature (UI, API, DB writes)?
    - Check: Is there a fallback/default behavior when flag is off?
  - **Database migrations:** If the feature added migrations:
    - Is there a reverse migration (down migration)?
    - Can the migration be reversed without data loss?
    - If migration is destructive (drops column/table): flag as ðŸ”´
  - **API changes:** If the feature changed API contracts:
    - Is the old API version still supported? (backward compatibility)
    - Are there consumers that will break if rolled back?
  - **No feature flag and no migration:** Rollback = revert the commit/PR
    - Verify the PR/branch can be cleanly reverted: `git revert --no-commit HEAD`

Show me: Rollback strategy summary, risk areas, recommended rollback steps.
Then: Flag any rollback blockers as ðŸ”´ per Remediation Rule.

===========================
CHECK 6: BUILD & DEPLOY VERIFICATION
===========================
- Verify the feature builds successfully in production mode:
  ```
  npm run build        # or: python -m build, docker build
  ```
- Check for:
  - Build warnings related to changed files
  - New environment variables required â€” are they documented and set in all environments?
  - If feature adds a new service/worker: is it in deployment config?
  - If feature adds a new route: is it in routing config and any reverse proxy/CDN config?

Show me: Build result, new env vars needed, deployment config changes needed.
Then: Propose fixes per Remediation Rule.

===========================
CHECK 7: PERFORMANCE IMPACT (optional â€” included in Full audit)
===========================
Skip for Quick audit. Include for Standard (if touching critical areas) and Full audit.

- Analyze changed files for performance concerns:
  - New database queries: Are they indexed? N+1 risk?
  - New API endpoints: Expected response time under load?
  - New frontend components: Bundle size impact? Lazy loaded?
  - New loops/iterations: Unbounded? Could scale poorly with data growth?
- If feature adds a new page/route:
  - Check bundle size impact: `npx next build` / webpack bundle analyzer
- Quick benchmark (if applicable):
  ```
  # API endpoint response time:
  curl -o /dev/null -s -w "Time: %{time_total}s\n" https://localhost:3000/new-endpoint
  ```

Show me: Performance concerns in new code, bundle size impact (if applicable).
Then: Propose optimizations per Remediation Rule.

===========================
CHECK 8: FEATURE COMPLETENESS (optional â€” included in Full audit)
===========================
Skip for Quick audit. Include for Full audit.

- **Analytics & tracking:**
  - Are analytics events added for the new feature? (page views, button clicks, conversions)
  - Are events firing correctly?
- **Error handling:**
  - Does the feature have proper error states? (loading, empty, error, success)
  - Are errors logged to the error tracking service (Sentry, etc.)?
- **Accessibility (if UI change):**
  - Keyboard navigation works on new UI elements?
  - Screen reader labels present?
  - Color contrast meets WCAG 2.1 AA?
- **Documentation:**
  - API docs updated for new endpoints?
  - README updated if setup steps changed?
  - Changelog entry added?

Show me: Completeness checklist with pass/fail per item.
Then: Propose fixes per Remediation Rule.

===========================
AUDIT SUMMARY
===========================
After all checks are complete, present:

```
============================================
FEATURE AUDIT SUMMARY
============================================
Feature: [name]
Branch: [branch]
Files Changed: [X] | Blast Radius: [Y] files
Audit Depth: [Quick/Standard/Full]
Duration: [X] minutes

ðŸ“Š Results:
| Check | Status | Findings |
|-------|--------|----------|
| 1. Code Quality | âœ…/âš ï¸/âŒ | X issues |
| 2. Test Coverage | âœ…/âš ï¸/âŒ | X% coverage on changed files |
| 3. Security | âœ…/âš ï¸/âŒ | X issues |
| 4. Regression | âœ…/âš ï¸/âŒ | X newly failing tests |
| 5. Rollback | âœ…/âš ï¸/âŒ | [ready/has blockers] |
| 6. Build & Deploy | âœ…/âš ï¸/âŒ | X issues |
| 7. Performance | âœ…/âš ï¸/âŒ/â­ï¸ | [findings or skipped] |
| 8. Completeness | âœ…/âš ï¸/âŒ/â­ï¸ | [findings or skipped] |

ðŸ”§ Remediation:
- Fixed during audit: [X] items
- Open ðŸ”´ Must Fix: [X] items
- Open ðŸŸ¡ Should Fix: [X] items
- Marked for manual review: [X] items

ðŸŽ¯ Verdict: READY TO MERGE âœ… / NEEDS FIXES âš ï¸ / DO NOT MERGE âŒ
============================================
```

**If READY TO MERGE:**
- *"Feature audit passed. Safe to merge and deploy."*
- If feature has a feature flag: *"Consider gradual rollout: enable for 10% â†’ 50% â†’ 100% of users over [timeframe]."*

**If NEEDS FIXES:**
- List remaining items with owners
- *"Address these items, then re-run the audit on the updated branch."*

**If DO NOT MERGE:**
- List blockers
- *"These critical issues must be resolved before this feature can be merged."*

===========================
CLEANUP
===========================
After audit is complete:
1. Remove backup files: `find . -name "*.feature-audit-backup" -delete`
2. If tests were written during audit, ensure they are committed
3. Update PR description with audit results summary (copy the summary block above)
```

## PROMPT END

---

## Usage Notes

- This audit complements `Pre_Release_Audit_Prompt.md` â€” use Pre-Release for initial launches, Feature Audit for ongoing changes.
- **When to use Feature Audit vs Pre-Release Audit:**
  - Adding a new button â†’ Feature Audit (Quick)
  - Adding a new payment method â†’ Feature Audit (Full, touches critical area)
  - Major refactor of 50% of codebase â†’ Pre-Release Audit
  - New microservice â†’ Pre-Release Audit for the new service, Feature Audit for integration points
- The audit runs on the feature branch BEFORE merging to main.
- Feature Audit can be run by the developer themselves â€” it doesn't require a separate auditor.
- For teams: consider making Feature Audit a required step in your PR review process.
- This prompt references safety rules from `Pre_Release_Audit_Prompt.md` â€” both files should be available in your project.
